Implementazione
Vogliamo disegnare un filesystem che si estenda in modo trasparente su piu' zone geografiche.
Ogni zona geografica deve essere in grado di operare in modo quasi completamente indipendentemente dalle altre, per fare in modo che i clients contenuti riescano a lavorare anche in caso le altre zone non siano disponibili e per non occupare troppo la banda tra zone.

Possiamo usare replicazione asincrona per tenere sincronizzate cluster di metadati tra due diverse zone, la primaria e la secondaria.
HopsFS utilizza le garanzie di consistenza fornite da NDB per implementare la consistenza a livello di operazione del filesystem e usando async replication non abbiamo queste garanzie
Quello che possiamo fare e' cercare di eseguire sul cluster primario tutte le operazioni che assolutamente necessitano di consistenza, cercare tecniche per evitare i conflitti dove possible e come ultima spiaggia risolvere i conflitti dove non si puo' evitare.
Le operazioni che richiedono consistency assolutamente sono principalmente quelle atte a mantenere la semantica single-writer, ossia quasi tutte le operazioni che hanno qualcosa a che fare con gli inodes e i blocchi.
Fortunatamente, come visto in workload sintetico spotify e validato da yahoo e facebook [papers], circa 95% delle operazioni sono in sola lettura e possono essere eseguite sul cluster locale.
Il restante 5% delle operazioni invece vanno eseguite sul cluster primario.
Dato che tutte le operazioni che committano transactions sono eseguite sul cluster primario per mantenere consistency nei leases e altra roba, e che sul cluster secondario eseguiamo solo operazioni di lettura, non ci saranno mai conflitti.
L'assunzione che tutte le operazioni vengano committate solo sul cluster primario non e' valida solo in caso i due clusters non riescano a comunicare tra di loro, una condizione che puo' essere causata da due eventi: 1) uno dei due clusters fallisce 2) i due clusters sono entrambi online ma non riescono a comunicare tra di loro: una condizione nota come network partition.
Network partition e' un fenomeno complesso che puo' manifestarsi in molti modi, ma in questo lavoro la definiamo come: la completa divisione dei due data centers in maniera tale che nessun nodo nel primo datacenter possa comunicare con qualsiasi nodo nel secondo e viceversa.
Oltre a semplificare la quantita' di casi da gestire, questa definizione e' supportata dal modello di datacenters geo-replicati proposto, dove tutta la comunicazione avviene attraverso una rete virtuale supportata da una connessione verso l'esterno.
In caso di fallimento di questa connessione, tutti i link tra i clusters faillirebbero.
In scenari reali, e come mostrato nel paper di google, i fallimenti di un cluster (o di un subset) sono abbastanza frequenti e spesso dovuti ad aggiornamenti del software che contengono errori.
Situazioni di split-brain sono invece meno frequenti, ma ripararle richiede tempo, come mostrato nel paper di microsoft sui link nei data-centers.
Determinare quando avvenga una condizione di split brain e' necessario per permettere ai clusters di adattarsi a questa situazione.

# detection di situazioni di split-brain
Per poter gestire situazioni dove i clusters sono divisi, il primo passo e' permettere ai namenodes di capire quando questa situazione si verifichi.
Per farlo proponiamo due procedure diverse, una da eseguire sui namenodes appartenenti al cluster primario e una da eseguire sui namenodes del cluster secondario.

## detection su primary
La detection di split-brain su cluster primario richiede l'adattamento della procedura di leader election gia' discussa in precedenza, aggiungendo ad ogni nodo un campo che dice se il nodo in questione appartenga al cluster primario o secondario.
A questo punto, per identificare una network partition eseguiamo periodicamente la seguente procedura:
* otteniamo la lista di namenodes attivi nel sistema dalla procedura di leader election (che fa da failure detector)
* se la lista contiene solo namenodes nella zona primaria allora assumiamo che siamo in una situazione di network partition
* se la lista contiene almeno un namenode della zona secondaria allora non siamo in una situazione di network partition (e invochiamo le procedure per uscirne se lo eravamo in precedenza)
La situazione dove il round di leader election non mostra alcun nodo della zona secondaria puo' avvenire solo se 1) tutti i nodi nella zona secondaria sono falliti (crash) 2) la connessione tra i nodi della zona secondaria e il metadata cluster primario e' fallita (split brain)

## detection sul secondary cluster
Per gestire la detection su cluster secondario non possiamo fare affidamento alla stessa procedura adottata dal primary, poiche' assumiamo che in tale situazione, la connessione verso il cluster primario sia compromessa.
In questo caso facciamo di nuovo affidamento sul failure detector distribuito contenuto nella procedura di leader election ma eseguiamo una di queste procedure sul cluster locale.
A questa procedura aggiungiamo un campo booleano su ogni namenode descriptor, un flag che indica se il nodo in questione abbia o meno una connessione al cluster primario.
Questa flag viene modificata ogniqualvolta la connessione al cluster primario sul namenode corrente fallisce (flag=false) o viene ripristinata (flag=true)
A questo punto eseguiamo la seguente procedura volta a determinare se almeno un nodo in questa zona ha la connessione al primary (nel qualcaso non esiste la condizione base di network partition):
* se il namenode dove eseguo la procedura e' connesso al cluster primario, non c'e' una network partition
* se questo non e' vero, ottieni la lista di namenodes dalla secondary-only leader election (che viene usata solo per il failure detector)
* se la lista contiene almeno un nodo attivo avente connessione verso il primary cluster, non c'e' una partition
* se la lista non contiene nodi attivi con connessione verso il primary: siamo in una situazione di n/w partition

A questo punto sia primary che secondary cluster hanno la capacita' di identificare network partitions indipendentemente ma non hanno modo di distinguere tra network partition o crash dell'altro cluster.
Questa capacita' puo' essere aggiunta fornendo un sistema ospitato in una terza zona, indipendente da entrambe, che agisca come tie-breaker e permetta ai sistemi di sapere, in modo consistent, quale delle due zone sia effettivamente available (o entrambe, se quello fosse il caso).
Tale sistema potrebbe essere implemento, a titolo di esempio, con un cluster di Zookeeper formato da tre nodi: uno nella primary zone, uno nella secondary zone, e uno in una terza zona indipendente da entrambe.
Con un meccanismo di questo tipo, un cluster non in grado di raggiungere il tie-breaker per qualsiasi motivo dovrebbe eseguire una procedura di shut-down, e solo clusters con accesso ad un quorum formato da almeno il tie-breaker piu' uno dei nodi sarebbero in grado di procedere.

In caso la situazione sia quella dove effettivamente solo uno dei clusters e' online allora:
* continuare come se nulla fosse in caso del primary
* eseguire tutte le operazioni importanti sul local cluster e agire come primary, nel caso del secondary cluster
In entrambi i casi, non si possono verificare conflitti, visto che il cluster restante opera a tutti gli effetti come un singolo cluster (con una policy di replicazione differente che sara' illustrata in \ref{sec:block-replication}).

# Split brain handling
Il caso in cui i due clusters siano entrambi running deve essere gestito con particolare cura per mantenere la single-writer semantic di HDFS.
Se permettessimo semplicemente ai due clusters di continuare a operare come due entita' differenti senza alcuna restrizione, anche ammettendo di assegnare block e inode id non conflittuali, potrebbero avvenire conflitti non facilmente risolvibili quando i cluster verranno unificati.

## Conflicts
In questo caso potremmo avere conflitti sia a livello di inode (nuovi files o cartelle creati con gli stessi nomi da entrambe le parti) che a livello di blocco.
I conflitti a livello di blocco sono particolarmente problematici perche' potremmo ritrovarci con blocchi su disco diversi a cui e' stato assegnato lo stesso ID.
Anche se questo verrebbe risolto al primo block report una volta che il cluster e' di nuovo unificato, poiche' il blocco creato sul master cluster vincerebbe e quelli creati dal secondario avrebbero checksum diverso e verrebbero quindi considerati corrotti, e' possibile creare una soluzione che prevenga i conflitti interamente.
Per comprendere la soluzione e' necessario capire come avviene l'assegnazione degli ID ai blocchi.
Siccome creare blocchi e' una operazione frequente, richiedere un nuovo ID al database ogni volta sarebe troppo costoso.
Il namenode richiede quindi una sequenza di ID liberi che utilizzera' per creare i prossimi blocchi.
Quando finisce questa sequenza di blocchi semplicemente ne richiedera' una nuova.
Se facciamo in modo che la sequenza di blocchi restituita contenga solo indici pari sul cluster primario e blocchi dispari sul cluster secondario, e siccome l'id del blocco piu' l'ID dell'inode sono la primary key della tabella dei blocchi, questo rende impossibile che due blocchi creati su due cluster distinti causino conflitti.

I conflitti sulla creazione di nuovi inodes, e quindi files e directories sono invece impossibile da prevenire, dato che un inode e' identificato solamente da il suo nome e dall'identificativo dell'inode padre.
Questo significa che se un file (o cartella) con lo stesso nome vengono creati sui due clusters, il conflitto e' inevitabile.
A questo punto l'unica cosa che possiamo fare e' gestire il conflitto.
Ci sono diverse possibili strategie, potremmo semplicemente decidere che la nuova cartella o il nuovo file creati sul cluster secondario sono da buttare (ignorando quindi i dati che sono stati posti nell'exception table) o possiamo decidere di risolvere il conflitto rinominando il file con <nome_1> o altro e mantenere il contenuto.

Con le tecniche sopracitate possiamo gestire in modo trasparente nuovi files e nuove directories ma non altre operazioni.
Sicome non abbiamo una tabella dei leases condivisa, e non possiamo quindi fare affidamento su questo meccanismo per sapere quali files esistenti stiano venendo modificati, permettere a clients di aggiungere dati ad un file potrebbe risultare nella situazione dove due clusters hanno due versioni dello stesso blocco, un conflitto irrisolvibile a meno di creare due intere copie del file o un meccanismo per permettere versioni divergenti dello stesso file.
Subtree operations, descritte nel paper di HopsFS (TODO: citale nel background su HopsFS [pagine gratis, yey]), a causa del loro uso dei lock per garantire consistency e poiche' generano un grande numero di grandi transactions, non sono permesse in questa modalita'
Infine sia delete che move sono disabilitate per simili ragioni.
Riassumendo, questa soluzione permette ai due data-centers separati durante una network partition di eseguire file reads e create di nuovi files e directories, operazioni che costituiscono oltre il 97% (TODO: verify) del workload di spotify e altri workload simili pubblicati da yahoo e facebook.

Una possibile alternativa da applicarsi in situazioni dove il workload e' significativamente diverso e richiede accesso a tutte le operazioni e' quello di istituire un sitema di arbitrio simile a quello di NDB e illustrato in section \ref{sec:ndb-arbitration}.
In questo caso, solo ad uno dei due clusters sarebbe permesso di continuare ad operare, mentre l'altro cluster sarebbe forzato ad andare in modalita' di sola lettura fino alla ricongiunzione.






