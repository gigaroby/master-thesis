The goal of this work is to plan for an extension to HopsFS that leverages the geographical replication capabilities built into MySQL Cluster and illustrated in Section \ref{sec:mysql-cluster} to build a geographically distributed file system that transparently appears to clients as a single name-space and maintains most of the consistency properties that clients expect.
Furthermore, clients running in or near the closest geographical location, the \emph{local} cluster, are expected to continue to function, possibly at reduced capacity, in case other, \emph{remote}, geographical locations fail or become unavailable for any reason.
This also implies that operations from clients in or near the local clusters should be processed in the local data center as much as possible to avoid saturation of the egress links that connect the different locations together.

Two data-centers are considered separate geographical locations if they are different, distant buildings that are serviced by different utilities such as power companies and internet service providers, and are therefore unlikely to be all affected by local catastrophic events such as loss of power or a localized earthquakes.
This requirement also influences network topology in that two machines in different geographical locations may only be able to connect to each other through a virtual network which connects to other data-centers through the external connection.
Due to the use of the external connection, packets travelling on the virtual network are subject to both additional overhead caused by the virtual networking protocols and routing on the open internet.
Such a topology implies that connection between machines running in different data-centers are subject to higher latency, often orders of magnitude higher, and lower, more expensive bandwidth compared to a connection between two machines in the same geographical zone.
A partial exception to this rule are cloud provider's Availability Zones (or just Zones depending on the provider specific terminology), which fulfill the requirements of different geographical locations but are connected by low-latency dedicated fibers and allow machines in two different zones to communicate with latency and bandwidth parameters similar to those of machines in the same zone.
They achieve this result by placing different data-center buildings just hundreds of kilometers from each other, connecting them to different power providers and ISPs and providing dedicated connection between the data centers themselves.
Cloud provider zones are, however, insignificant to our goal as a system designed to run in the former scenario will only perform better when deployed in the latter.

As shown in Section \ref{sec:hdfs} and \ref{sec:hopsfs}, the HopsFS architecture involves three main components:
\begin{enumerate}
    \item a set of namenodes which process client and datanode RPC requests as well as performing background periodic maintenance tasks such as re-replication of blocks which keep the cluster  in the correct state,
    \item a set of datanodes which store block data and checksums and report their status to the namenodes using heartbeats, and
    \item a metadata storage cluster which stores and handles modification of the cluster metadata by the namenodes.
\end{enumerate}
In order to allow clients to perform operations on the local cluster, which is one of the key objectives of the project, each of the clusters needs
\begin{inparaenum}
    \item a complete copy of all metadata,
    \item a complete copy of all filesystem data, and
    \item running instances of all the components required for the system to function on its own.
\end{inparaenum}.
If this were not true, operations on the local cluster would require very expensive connections to a remote cluster, operations that would fail and render the local cluster inoperable in the event of remote cluster failure.
Replicating the infrastructural components is by far the simplest task, as it only involves the deployment of a complete cluster in the other geographical location plus some configuration to connect the clusters.
Management of filesystem metadata and blocks are, however, very complex problems and the focus of this thesis.